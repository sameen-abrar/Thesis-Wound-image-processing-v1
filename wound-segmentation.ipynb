{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-12-03T00:51:08.089295Z",
     "iopub.status.busy": "2021-12-03T00:51:08.08853Z",
     "iopub.status.idle": "2021-12-03T00:51:08.093766Z",
     "shell.execute_reply": "2021-12-03T00:51:08.092921Z",
     "shell.execute_reply.started": "2021-12-03T00:51:08.089258Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/code/mohamadtaher/wound-segmentation/notebook\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T00:51:08.120029Z",
     "iopub.status.busy": "2021-12-03T00:51:08.119819Z",
     "iopub.status.idle": "2021-12-03T00:51:08.842861Z",
     "shell.execute_reply": "2021-12-03T00:51:08.841349Z",
     "shell.execute_reply.started": "2021-12-03T00:51:08.120004Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "original_image = \"../input/wound-data/train/images/0011.png\"\n",
    "label_image_semantic = \"../input/wound-data/train/labels/0011.png\"\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8), constrained_layout=True)\n",
    "\n",
    "axs[0].imshow( Image.open(original_image))\n",
    "axs[0].grid(False)\n",
    "\n",
    "label_image_semantic = Image.open(label_image_semantic)\n",
    "label_image_semantic = np.asarray(label_image_semantic)\n",
    "print(label_image_semantic.shape)\n",
    "axs[1].imshow(label_image_semantic)\n",
    "axs[1].grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T00:51:08.845198Z",
     "iopub.status.busy": "2021-12-03T00:51:08.844614Z",
     "iopub.status.idle": "2021-12-03T00:51:08.867695Z",
     "shell.execute_reply": "2021-12-03T00:51:08.866961Z",
     "shell.execute_reply.started": "2021-12-03T00:51:08.845153Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_dir = \"../input/wound-data/train/images/\"\n",
    "target_dir = \"../input/wound-data/train/labels/\"\n",
    "\n",
    "val_dir = \"../input/wound-data/validation/images\"\n",
    "val_target_dir = \"../input/wound-data/validation/labels\"\n",
    "\n",
    "test_dir ='../input/wound-data/test/images'\n",
    "img_size = (256, 256)\n",
    "num_classes = 2\n",
    "batch_size = 32\n",
    "test_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(test_dir, fname)\n",
    "        for fname in os.listdir(test_dir)\n",
    "        if fname.endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "        if fname.endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(val_dir, fname)\n",
    "        for fname in os.listdir(val_dir)\n",
    "        if fname.endswith(\".png\")\n",
    "    ]\n",
    ")\n",
    "val_target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(val_target_dir, fname)\n",
    "        for fname in os.listdir(val_target_dir)\n",
    "        if fname.endswith(\".png\") and not fname.startswith(\".\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "print(\"Number of val samples:\", len(val_img_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "\n",
    "#I will use keras sequence class to bulid custom loader for our data\n",
    "class WoundGen(keras.utils.Sequence):\n",
    "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
    "\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
    "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
    "#             for i in np.array(img):\n",
    "#                 print(i)\n",
    "            display(img)\n",
    "#             print(max(np.array(img)) ,min(np.array(img)))\n",
    "#             y[j] = np.expand_dims(img, 2)\n",
    "            y[j] = (y[j]/255)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T00:51:08.884115Z",
     "iopub.status.busy": "2021-12-03T00:51:08.883615Z",
     "iopub.status.idle": "2021-12-03T00:51:09.309585Z",
     "shell.execute_reply": "2021-12-03T00:51:09.308787Z",
     "shell.execute_reply.started": "2021-12-03T00:51:08.884077Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def get_model(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "\n",
    "    ### [First half of the network: downsampling inputs] ###\n",
    "\n",
    "    # Entry block\n",
    "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    previous_block_activation = x  # Set aside residual\n",
    "\n",
    "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
    "            previous_block_activation\n",
    "        )\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    ### [Second half of the network: upsampling inputs] ###\n",
    "\n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "        # Project residual\n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
    "        x = layers.add([x, residual])  # Add back residual\n",
    "        previous_block_activation = x  # Set aside next residual\n",
    "\n",
    "    # Add a per-pixel classification layer\n",
    "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Free up RAM in case the model definition cells were run multiple times\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# Build model\n",
    "model = get_model(img_size, num_classes)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = WoundGen(batch_size, img_size, input_img_paths, target_img_paths)\n",
    "val_gen = WoundGen(batch_size, img_size, val_img_paths, val_target_img_paths)\n",
    "test_gen = WoundGen(batch_size, img_size, test_img_paths, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = train_gen[0]\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "# min(y.all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T00:51:09.675612Z",
     "iopub.status.busy": "2021-12-03T00:51:09.674841Z",
     "iopub.status.idle": "2021-12-03T00:51:09.679Z",
     "shell.execute_reply": "2021-12-03T00:51:09.678321Z",
     "shell.execute_reply.started": "2021-12-03T00:51:09.675572Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install libhdf5\n",
    "# !pip install h5py\n",
    "# kk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-12-03T00:51:09.680926Z",
     "iopub.status.busy": "2021-12-03T00:51:09.680165Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
    "\n",
    "# callbacks = [\n",
    "#     keras.callbacks.ModelCheckpoint(\"wound_segmentation.h5\", save_best_only=True)\n",
    "# ]\n",
    "\n",
    "# Train the model, doing validation at the end of each epoch.\n",
    "epochs = 15\n",
    "model.fit(train_gen, epochs=epochs, validation_data=val_gen)#, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = np.array(Image.open(\"../input/wound-data/test/images/1016.png\"))\n",
    "test = np.array(Image.open(\"../input/wound-data/train/images/0011.png\"))\n",
    "\n",
    "test = test.reshape(1,512,512,3)\n",
    "label  = np.array(model.predict(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "from PIL import ImageOps\n",
    "def display_mask(preds,i=0):\n",
    "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "    mask = np.argmax(preds[i], axis=-1)\n",
    "    mask = np.expand_dims(mask, axis=-1)\n",
    "    img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "#     display(img)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_mask(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_images = \"../input/wound-data/test/images\"\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 8), constrained_layout=True)\n",
    "\n",
    "axs[0].imshow( test[0])\n",
    "axs[0].grid(False)\n",
    "\n",
    "mask = np.argmax(label[0], axis=-1)\n",
    "mask = np.expand_dims(mask, axis=-1)\n",
    "img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "axs[1].imshow(img)\n",
    "axs[1].grid(False)\n",
    "\n",
    "# val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
    "# val_preds = model.predict(val_gen)\n",
    "\n",
    "\n",
    "# def display_mask(i):\n",
    "#     \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
    "#     mask = np.argmax(val_preds[i], axis=-1)\n",
    "#     mask = np.expand_dims(mask, axis=-1)\n",
    "#     img = PIL.ImageOps.autocontrast(keras.preprocessing.image.array_to_img(mask))\n",
    "#     display(img)\n",
    "\n",
    "\n",
    "# # Display results for validation image #10\n",
    "# i = 10\n",
    "\n",
    "# # Display input image\n",
    "# display(Image(filename=val_input_img_paths[i]))\n",
    "\n",
    "# # Display ground-truth target mask\n",
    "# img = PIL.ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
    "# display(img)\n",
    "\n",
    "# # Display mask predicted by our model\n",
    "# display_mask(i)  # Note that the model only sees inputs at 150x150."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfp3.9]",
   "language": "python",
   "name": "conda-env-tfp3.9-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
